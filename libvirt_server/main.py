import asyncio
import shutil
import time

from dotenv import load_dotenv

from utils.env_utils import get_env_var
from utils.logger import logger, set_log_file, set_log_format, set_log_level

from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_tracing_disabled
from openai.types.responses import ResponseTextDeltaEvent, ResponseStreamEvent
from openai import OpenAIError
from agents.mcp import MCPServerStreamableHttp
from agents.model_settings import ModelSettings

from prompt_toolkit import PromptSession
from prompt_toolkit.completion import WordCompleter

load_dotenv()
set_tracing_disabled(disabled=True)

set_log_level("DEBUG")
set_log_file(file_name="agent_log.log")
set_log_format(format_type="text")

openai_client = AsyncOpenAI(
    api_key=get_env_var("API_KEY"),
    base_url=get_env_var("API_URL")
)

extra_body = {
    "chat_template_kwargs": {"enable_thinking": True},
}

# å…¨å±€ä¿æŒå†å²
chat_history = []

async def run(agent: Agent, message: dict) -> ResponseStreamEvent:
    logger.debug(f"Running: {message}")
    start_time = time.time()
    result = Runner.run_streamed(starting_agent=agent, input=message)
    first_response = True

    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            if first_response:
                first_response = False
                logger.info(f"â±ï¸ é¦–ä¸ªå“åº”è€—æ—¶: {time.time() - start_time:.2f} ç§’")
            print(event.data.delta, end="", flush=True)

    print()
    return result.final_output

async def main():
    global chat_history
    
    session = PromptSession()

    # åˆ›å»º MCP Streamableè¿æ¥
    async with MCPServerStreamableHttp(
        name="Streamable HTTP Python Server",
        params={"url": "http://localhost:8000/mcp"},
        cache_tools_list=True
    ) as server:

        agent = Agent(
            name="Assistant",
            instructions=
"""
ä½ æ˜¯ä¸€åè™šæ‹Ÿæœºæ™ºèƒ½ç®¡ç†åŠ©æ‰‹ï¼Œä¸“æ³¨äºå¸®åŠ©ç”¨æˆ·**æŸ¥è¯¢å’Œæ§åˆ¶è™šæ‹Ÿæœºä¸å·èµ„æº**ã€‚
ä½ æ‹¥æœ‰ä¸€å¥—ä¸°å¯Œçš„å·¥å…·æ¥å®Œæˆè™šæ‹Ÿæœºç®¡ç†ç›¸å…³ä»»åŠ¡ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¯·æ±‚ï¼Œ**å‡†ç¡®é€‰æ‹©åˆé€‚çš„å·¥å…·**è°ƒç”¨ã€‚

### ğŸ” ä½ çš„èƒ½åŠ›åŒ…æ‹¬ï¼ˆä½†ä¸é™äºï¼‰ï¼š
- æŸ¥è¯¢è™šæ‹Ÿæœºçš„åŸºæœ¬ä¿¡æ¯ã€IPã€çŠ¶æ€ã€èµ„æºå ç”¨ï¼ˆCPUã€ç£ç›˜ã€ç½‘ç»œç­‰ï¼‰
- å¯åŠ¨ã€å…³é—­ã€æš‚åœã€é‡å¯ã€å…‹éš†ã€åˆ é™¤è™šæ‹Ÿæœº
- æŸ¥è¯¢å’Œç®¡ç†å­˜å‚¨æ± ä¸­çš„å·ï¼ˆåˆ—å‡ºå·ã€åˆ›å»ºã€å…‹éš†ã€åˆ é™¤ã€ä¸Šä¼ ã€ä¸‹è½½ç­‰ï¼‰
- è·å–å®¿ä¸»æœºçš„æ€»ä½“èµ„æºçŠ¶æ€å’Œè™šæ‹Ÿæœºç»Ÿè®¡ä¿¡æ¯

### ğŸ§  å·¥å…·ä½¿ç”¨å»ºè®®ï¼š
1. **å°½é‡ä½¿ç”¨èšåˆå·¥å…·**ï¼ˆå¦‚ï¼š`fetch_vm_full_stats`ï¼‰æ¥æé«˜æ•ˆç‡ï¼Œé¿å…å¤šä¸ªç‹¬ç«‹æŸ¥è¯¢ï¼›
2. **æ”¯æŒç»„åˆè°ƒç”¨**å¤šä¸ªå·¥å…·è§£å†³å¤æ‚é—®é¢˜ï¼ˆå¦‚ï¼šåŒæ—¶æŸ¥è¯¢çŠ¶æ€å’Œç£ç›˜ä¿¡æ¯ï¼‰ï¼›
3. å½“ç”¨æˆ·çš„è¯·æ±‚ä¸æ¶‰åŠæ“ä½œæˆ–æ•°æ®æŸ¥è¯¢ï¼ˆå¦‚é—®å€™è¯­ã€ç¡®è®¤ã€å»ºè®®ç­‰ï¼‰ï¼Œ**ä¸è°ƒç”¨å·¥å…·ï¼Œç›´æ¥å›å¤å³å¯**ï¼›
4. å¦‚æœç”¨æˆ·åªæåˆ°â€œè™šæ‹Ÿæœºåâ€ä½†æœªæ˜ç¡®æ“ä½œï¼Œä½ å¯ä»¥å…ˆæŸ¥è¯¢åŸºæœ¬çŠ¶æ€ï¼ˆå¦‚`fetch_vm_state`ï¼‰å†æ¨æ–­æ„å›¾ï¼›
5. å½“ä½ è°ƒç”¨å®Œå·¥å…·åï¼Œè¯·å°†**ç»“æœä»¥ Markdown è¡¨æ ¼å½¢å¼è¿”å›**ï¼Œå¹¶åŠ ç®€è¦è¯´æ˜ï¼›
6. å½“ç»“æœä¸ºç©ºæˆ–å·¥å…·è¿”å›å¤±è´¥ï¼Œè¯·**æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·åŸå› **ï¼ˆå¦‚æ‰¾ä¸åˆ°è™šæ‹Ÿæœºã€æ“ä½œå¤±è´¥ç­‰ï¼‰ï¼›
7. å½“ç”¨æˆ·è¯´å‚æ•°è‡ªå®šæ—¶ï¼Œè¯·å…ˆæ£€æŸ¥å…ˆå‰æœ‰æ²¡æœ‰ä½¿ç”¨ç›¸åŒçš„å·¥å…·æŸ¥è¯¢è¿‡ç±»ä¼¼çš„å‚æ•°ï¼Œå¦‚æœæœ‰ï¼Œè¯·å°½é‡**é¿å…ä½¿ç”¨**ã€‚å†è€…ï¼Œåœ¨æ‰§è¡Œå‰å¯ä»¥ä½¿ç”¨ç›¸å…³å·¥å…·å…ˆåˆ—å‡ºå‚æ•°æ˜¯å¦å­˜åœ¨

### ğŸ“‹ è¾“å‡ºæ ¼å¼è¯´æ˜ï¼š
- è‹¥è°ƒç”¨äº†å·¥å…·ï¼Œä¼˜å…ˆä»¥ Markdown è¡¨æ ¼è¿”å›ç»“æœï¼›
- è‹¥æ— æ•°æ®ï¼Œè¾“å‡ºï¼šâ€œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç¡®è®¤è™šæ‹Ÿæœº/å·åç§°æ˜¯å¦æ­£ç¡®ã€‚â€
- è‹¥ä¸ºç®€å•é—®ç­”ç±»é—®é¢˜ï¼Œå¦‚â€œä½ æ˜¯è°â€ã€â€œä½ å¥½â€ç­‰ï¼Œè¯·è‡ªç„¶å›ç­”ï¼Œå¹¶é™„åŠ ä¸€å¥ï¼š
â€œæˆ‘æ˜¯è™šæ‹Ÿæœºæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·é—®æˆ‘èƒ½ä¸ºä½ åšäº›ä»€ä¹ˆï¼ŸCialloï½(âˆ ãƒ»Ï‰<)âŒ’â˜…â€

### é¡»çŸ¥ï¼š
- è¯·ä¸è¦è¯´è°ï¼Œè¯·ä¸è¦è‡ªä½œä¸»å¼ è„‘éƒ¨æ•°æ®ï¼Œé™¤éç”¨æˆ·å…è®¸ä½ æ ¹æ®å·¥å…·å†…å®¹ç”Ÿæˆç›¸å¯¹åº”çš„å‡½æ•°ï¼›
- è¯·åœ¨æœ‰é™çš„ç¯å¢ƒä¸­å°½å¯èƒ½å®Œæˆç”¨æˆ·çš„çš„è¯·æ±‚ï¼Œè‹¥é‡åˆ°è¶…å‡ºè‡ªèº«ä¹‹å¤–çš„é—®é¢˜ï¼Œå¯ä»¥è¯¢é—®ç”¨æˆ·ï¼Œç›´åˆ°é—®é¢˜å®Œæˆï¼›
- è¯·æ ¹æ®ä»¥ä¸Šè¯´æ˜ï¼Œåˆç†è°ƒç”¨å·¥å…·å¹¶ç”Ÿæˆç»“æœã€‚
""",
            mcp_servers=[server],
            model=OpenAIChatCompletionsModel(
                model=get_env_var("LLM_MODEL"),
                openai_client=openai_client
            ),
            model_settings=ModelSettings(
                max_tokens=8192,
                temperature=0.6,
                top_p=0.9,
                truncation='auto',
                extra_body=extra_body,
                tool_choice='auto'
            ),
        )

        while True:
            try:
                completer = WordCompleter([
                    'ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ',
                    'ä½ èƒ½åšä»€ä¹ˆï¼Ÿ',
                    'What can you do ï¼Ÿ',
                    'ç°åœ¨åŒ—äº¬æ—¶é—´',
                    'What time is it in Beijing now.',
                    'å½“å‰æœ‰å“ªäº›è™šæ‹Ÿæœºï¼Ÿ',
                    'How many virtual machines are there on the host ?',
                    'å½“å‰æ­£åœ¨è¿è¡Œçš„æœ‰å“ªäº›è™šæ‹Ÿæœºï¼Ÿ',
                    'Which virtual machines are currently running on the host ?',
                    'å½“å‰æœ‰å“ªäº›è™šæ‹Ÿç½‘ç»œï¼Ÿ',
                    'How many virtual networks are there on the host ?',
                    'å½“å‰æœ‰å“ªäº›è™šæ‹Ÿç½‘æ¡¥ï¼Ÿ',
                    'How many bridge interfaces are there on the host ?',
                    'å½“å‰æœ‰å“ªäº›å­˜å‚¨æ± ï¼Ÿ',
                    'How many storage pools are there on the host ?',
                    'å½“å‰å­˜å‚¨æ± ä¸­æœ‰å¤šå°‘å·ï¼Ÿ',
                    'How many volumes are there in the storage pool ?',
                    'ä½¿ç”¨"è™šæ‹Ÿæœºåç§°"æŸ¥è¯¢è¿™ä¸ªè™šæ‹Ÿæœºä¿¡æ¯ï¼š',
                    'Lookup this virtual machine by name: ',
                    'ä½¿ç”¨è™šæ‹Ÿæœº"UUID"æŸ¥è¯¢è¿™ä¸ªè™šæ‹Ÿæœºä¿¡æ¯ï¼š',
                    'Lookup this virtual machine by UUID: ',
                    'quit',
                    'exit'
                    ], 
                    ignore_case=True
                )
                user_input = await session.prompt_async("Input your questions (or 'exit/quit' to exit): >>> ", completer=completer)

                if not user_input:
                    continue

                if user_input in {"exit", "quit"}:
                    print("Bye!")
                    break

                messages = chat_history + [{"role": "user", "content": user_input}]

                message_output = await run(agent, messages)
                chat_history.append({"role": "assistant", "content": message_output})

            except OpenAIError:
                raise
            except KeyboardInterrupt:
                print("Exit.")
                break
            except EOFError:
                print("Exit.")
                break

if __name__ == "__main__":
    if not shutil.which("uv"):
        raise RuntimeError(
            "uv command is not installed, Please install it: https://docs.astral.sh/uv/getting-started/installation"
        )
    asyncio.run(main())
